# Malware Analysis Orchestration with AI

This project demonstrates how to build AI-powered chatbots for malware analysis using LangGraph, LangChain, and various LLM providers. The chatbots integrate with Binary Ninja through the [Lattice framework](https://github.com/Invoke-RE/binja-lattice-mcp/tree/main) to provide automated binary analysis capabilities.

## Overview

The project contains three different chatbot implementations, each showcasing different approaches to AI-powered malware analysis:

1. **Basic Tool-Enabled Chatbot** - Simple tool calling with local Ollama
2. **ReAct Agent with Gemini** - Advanced reasoning with Google's Gemini model
3. **ReAct Agent with Local Ollama** - Advanced reasoning with local Ollama model

## Features

- **Binary Analysis Tools**: Integration with Binary Ninja for disassembly, pseudocode, and function analysis
- **AI-Powered Reasoning**: Uses large language models to understand and analyze binary code
- **Tool Orchestration**: LangGraph-based workflow management for complex analysis tasks
- **Multiple LLM Support**: Works with both cloud-based (Gemini) and local (Ollama) models
- **Conversation Memory**: Maintains context across multiple analysis sessions

## Prerequisites

- Python 3.8+
- Binary Ninja with Lattice framework
- Ollama (for local models)
- Google Cloud API key (for Gemini)

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd malware-analysis-orchestration-with-ai
```

2. Install dependencies:
```bash
pip install langgraph langchain langchain-ollama langchain-google-genai
```

3. Set up environment variables:
```bash
export GEMINI="your-google-api-key-here"
```

4. Install and configure Ollama (for local models):
```bash
# Follow instructions at https://ollama.ai
ollama pull qwen-3.0-a3b
```

## Usage

### 1. Basic Tool-Enabled Chatbot (`chatbot-basic-local-ollama.py`)

A simple implementation that demonstrates basic tool calling capabilities using a local Ollama model.

**Features:**
- Direct tool execution based on AI model requests
- Local model inference with Ollama
- Basic conversation flow management

**Usage:**
```bash
python chatbot-basic-local-ollama.py
```

**Example Interaction:**
```
User: Analyze the binary at address 0x1000
Assistant: I'll analyze the binary starting at address 0x1000. Let me examine the disassembly and pseudocode...
```

### 2. ReAct Agent with Gemini (`chatbot-ReAct-gemini.py`)

An advanced implementation using Google's Gemini model with ReAct (Reasoning + Acting) capabilities for sophisticated binary analysis.

**Features:**
- Advanced reasoning and planning capabilities
- Cloud-based Gemini 2.0 Flash model
- Automatic tool selection and execution
- Expert reverse engineering prompt

**Usage:**
```bash
export GEMINI="your-api-key"
python chatbot-ReAct-gemini.py
```

**Example Interaction:**
```
User: What does this function do and are there any suspicious patterns?
Assistant: Let me analyze this function step by step. First, I'll examine the disassembly to understand the control flow...
```

### 3. ReAct Agent with Local Ollama (`chatbot-ReAct-local-ollama.py`)

Similar to the Gemini version but uses a local Ollama model for privacy and offline analysis.

**Features:**
- Same ReAct capabilities as Gemini version
- Local model inference for privacy
- Offline analysis capabilities
- Customizable model selection

**Usage:**
```bash
python chatbot-ReAct-local-ollama.py
```

## Architecture

The project uses LangGraph to create a state machine that manages:

1. **User Input Processing**: Handles natural language queries
2. **AI Reasoning**: LLM processes the query and determines needed tools
3. **Tool Execution**: Calls appropriate Binary Ninja analysis functions
4. **Response Generation**: Formats results for user consumption
5. **State Management**: Maintains conversation context and memory

## Configuration

### Model Selection

- **Ollama Models**: Change the model name in the `ChatOllama()` constructor
- **Gemini Models**: Modify the model parameter in `ChatGoogleGenerativeAI()`
- **Temperature**: Adjust reasoning creativity (0 = deterministic, higher = more creative)

### Lattice Configuration

Update the `lattice_session` variable to connect to your Binary Ninja instance:

```python
lattice_session = "your-lattice-session-id"
```

## Troubleshooting

### Common Issues

1. **Lattice Connection Failed**: Ensure Binary Ninja is running, the lattice plugin is running and you've entered your API keys accordingly
3. **API Key Error**: Verify your Google Cloud API key is set correctly
4. **Tool Execution Failed**: Check that the binary is loaded in Binary Ninja and check with the LLM that the tools are visible

## License

This project is licensed under the terms specified in the [LICENSE.md](LICENSE.md) file.
